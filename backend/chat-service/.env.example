# Embedding Model
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2

# Vector Database
VECTORDB_PERSIST_DIRECTORY=data/vectordb
VENDOR_DATA_DIRECTORY=sample-data

# ========================================
# LLM Configuration
# ========================================

# Option 1: Use Google Gemini (Cloud) - DEFAULT
# Set USE_LOCAL_LLM=false to use Gemini
USE_LOCAL_LLM=false
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL_NAME=gemini-2.5-flash

# Option 2: Use Ollama (Local)
# Set USE_LOCAL_LLM=true to use local Ollama
# Make sure Ollama is running: ollama serve
# USE_LOCAL_LLM=true
LOCAL_LLM_BASE_URL=http://localhost:11434
LOCAL_LLM_MODEL=phi3:mini
# Alternatives: mistral:latest, llama3.1:8b, gemma2:9b
LOCAL_LLM_TEMPERATURE=0.7
LOCAL_LLM_MAX_TOKENS=512

# ========================================
# How to switch between Local and Cloud:
# ========================================
# For LOCAL testing:  USE_LOCAL_LLM=true
# For PRODUCTION:     USE_LOCAL_LLM=false
